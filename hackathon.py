# -*- coding: utf-8 -*-
"""Hackathon.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14qQwliH6lEC7OjOaZmZbJwSsBBLh8QvC
"""

# Clean the file manually before reading
with open("bank.csv", "r", encoding="utf-8") as f:
    lines = f.readlines()

# Remove extra quotes and save to new file
with open("bank-full-cleaned.csv", "w", encoding="utf-8") as f:
    for line in lines:
        # Only replace double quotes with single to keep column delimiters intact
        cleaned_line = line.replace('""', '"')
        f.write(cleaned_line)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# âœ… Load cleaned CSV
df = pd.read_csv('bank-full-cleaned.csv', delimiter=';', engine='python')

print(df.head())
print(df.info())
print(df.describe())
print(df['y'].value_counts())

# Frequency count for categorical variables
print(df['job'].value_counts())
print(df['marital'].value_counts())
print(df['education'].value_counts())

# Check if any values are missing
print(df.isnull().sum())

print((df == "unknown").sum())

print(df['y'].value_counts(normalize=True))

import seaborn as sns
import matplotlib.pyplot as plt

# Age distribution
sns.histplot(df['age'], bins=20, kde=True)
plt.title("Age Distribution")
plt.show()

# Term deposit subscription by job type
plt.figure(figsize=(10, 4))
sns.countplot(data=df, x='job', hue='y')
plt.xticks(rotation=45)
plt.title("Subscription by Job")
plt.show()

# Convert binary columns to 0/1
binary_cols = ['default', 'housing', 'loan', 'y']
df[binary_cols] = df[binary_cols].replace({'yes': 1, 'no': 0})

# One-hot encode categorical features
df = pd.get_dummies(df, columns=['job', 'marital', 'education', 'contact', 'month', 'poutcome'], drop_first=True)

# Check for missing values
print(df.isnull().sum())

# Optional: Drop 'duration' if you want to avoid leakage
# df.drop('duration', axis=1, inplace=True)

from sklearn.model_selection import train_test_split

X = df.drop('y', axis=1)
y = df['y']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC

models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, class_weight='balanced'),
    "Decision Tree": DecisionTreeClassifier(class_weight='balanced'),
    "Random Forest": RandomForestClassifier(class_weight='balanced'),
    "Gradient Boosting": GradientBoostingClassifier(),  # doesn't support class_weight directly
    "SVM": SVC(probability=True, class_weight='balanced')
}

for name, model in models.items():
    model.fit(X_train, y_train)
    print(f"{name} trained.")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

for name, model in models.items():
    y_pred = model.predict(X_test)
    print(f"\n{name} Performance:")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Precision:", precision_score(y_test, y_pred))
    print("Recall:", recall_score(y_test, y_pred))
    print("F1 Score:", f1_score(y_test, y_pred))
    print(classification_report(y_test, y_pred))

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
}

grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=3, scoring='f1')
grid.fit(X_train, y_train)

print("Best Parameters:", grid.best_params_)
best_model = grid.best_estimator_

pip install streamlit

# app.py
import streamlit as st
import matplotlib.pyplot as plt

st.title("Model Performance Dashboard")

scores = {'Logistic Regression': 0.87, 'Decision Tree': 0.85, 'Random Forest': 0.87, 'Gradient Boosting': 0.88, 'SVM': 0.83}
models = list(scores.keys())
f1_scores = list(scores.values())

fig, ax = plt.subplots()
ax.barh(models, f1_scores)
ax.set_xlabel('F1 Score')
st.pyplot(fig)

st.write("Best performing model: Gradient Boosting (0.92)")